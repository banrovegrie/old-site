## Universal Approximation

Perceptrons are the simplest form of neural networks.

![Untitled|center|500](Algorithm%20Design/images/Geometric%20Deep%20Learning%20e1980a7ab01e4c269b539b68f3daf60b/Untitled.png)

Now, MLPs or multi-layer perceptrons are also called universal approximators.

![Untitled|center|500](Algorithm%20Design/images/Geometric%20Deep%20Learning%20e1980a7ab01e4c269b539b68f3daf60b/Untitled%201.png)

## The Curse of Dimensionality

The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.

In case of machine learning, the number of learning samples increases exponentially with increasing dimensions.

## Graphs

![Untitled|center|250](Algorithm%20Design/images/Geometric%20Deep%20Learning%20e1980a7ab01e4c269b539b68f3daf60b/Untitled%202.png)

Graph functions and node functions are permutation independent. Graph Theorists argue that GNNs are just special cases of graph isomorphism test(s) like WL test.